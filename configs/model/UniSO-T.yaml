_target_: src.models.UniSO-T.OmniPredModule

encoder_model:
  _target_: transformers.T5EncoderModel
  config:
    _target_: transformers.T5Config
    vocab_size: 32128
    num_layers: 6
    d_kv: 32
    d_model: 384
    d_ff: 512

decoder_model:
  _target_: transformers.models.t5.modeling_t5.T5Stack
  _partial_: true
  config:
    _target_: transformers.T5Config
    # vocab_size: 32128
    num_layers: 6
    d_kv: 32
    # d_model:
    d_ff: 512
    is_decoder: true  
    use_cache: true  

input_tokenizer: ${data.input_tokenizer}
output_tokenizer: ${data.output_tokenizer}

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 1e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

scheduler:
  _target_: transformers.get_scheduler
  _partial_: true
  name: "cosine"
  num_warmup_steps: 1000
  num_training_steps: ${trainer.max_epochs} 

compile: true